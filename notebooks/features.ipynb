{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0f2ab19-d193-4dd7-9c9a-2ea66006e9ee",
   "metadata": {},
   "source": [
    "## Label conversation chunks & build features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f533508e-841e-44ed-baef-a4a65a0dea00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import global modules\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from yaml import safe_load\n",
    "import google.oauth2.credentials\n",
    "from google.cloud import bigquery\n",
    "from IPython.core.display import HTML\n",
    "from IPython.core.display import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Set global vars\n",
    "pth_project = Path(os.getcwd().split('notebooks')[0])\n",
    "pth_data = pth_project / 'data'\n",
    "pth_util_data = pth_project / 'core' / 'utils' / 'data'\n",
    "pth_queries = pth_project / 'core' / 'utils' / 'queries' / 'common'\n",
    "pth_creds = pth_project / 'conf' / 'local' / 'project_config.yaml'\n",
    "sys.path.insert(0, str(pth_project))\n",
    "d_config = safe_load(pth_creds.open())\n",
    "d_params = safe_load((pth_project / 'core' / 'parameters' / 'movers.yaml').open())\n",
    "\n",
    "# import local modules\n",
    "from core.utils.gcp import connect_bq_services, connect_storage_services\n",
    "from core.etl.movers.extract import extract_all_model_data\n",
    "from core.etl.load import load_examples_to_datahub\n",
    "from core.etl.extract import extract_bq_data\n",
    "from core.etl.text.transform import sub_tokens, get_match_regex, is_match_regex\n",
    "from core.etl.movers.transform.features import process_conv, extract_convs_features\n",
    "from core.models.movers import MoveDeepClf\n",
    "\n",
    "# Connect to google services\n",
    "bq_client = connect_bq_services(d_config['gcp-project-name'])\n",
    "storage_client = connect_storage_services(d_config['gcp-project-name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bd1b14-c570-4cf9-9ec4-e9e30d24c61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edacf90b",
   "metadata": {},
   "source": [
    "#### 1. Extract data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f2f3f0",
   "metadata": {},
   "source": [
    "##### A Extract raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117b4e53-267b-4908-9fdf-52dc57c6b6c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract conversation and sentences \n",
    "df_sentences = pd.read_csv(pth_data / 'extract' / 'sentences.csv', index_col=None)\n",
    "\n",
    "# Add a unique chunk_id to all sentences \n",
    "df_ori_sentences = df_sentences.sort_values(by=['call_convrstn_id', 'sntnce_ts'])\\\n",
    "    .assign(chunk_id=np.arange(len(df_sentences)))\n",
    "\n",
    "# Extract model data\n",
    "d_regex, l_tags, d_stopwords, intent_detector = extract_all_model_data(\n",
    "    pth_util_data, split_regex=True, **d_params['model']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31603a-7432-452f-9e39-064eeb044414",
   "metadata": {},
   "source": [
    "#### 2. Preliminary sizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43798427-a71e-48e5-935d-7048da0ac987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all family match\n",
    "l_matches = []\n",
    "for _, row in df_ori_sentences.iterrows(): \n",
    "    l_move_match = get_match_regex(row['sntnce'], d_regex['movers'])\n",
    "    \n",
    "    if not l_move_match or row['sntnce_partcpnt_role'] != 'END_USER':\n",
    "        continue\n",
    "    \n",
    "    l_cancel_match = get_match_regex(row['sntnce'], d_regex['cancel'])\n",
    "    l_holiday_match = get_match_regex(row['sntnce'], d_regex['holiday'])\n",
    "    l_expression_match = get_match_regex(row['sntnce'], d_regex['expression'])\n",
    "    l_things_match = get_match_regex(row['sntnce'], d_regex['things'])\n",
    "    l_negation_regex = get_match_regex(row['sntnce'], d_regex['negation'])\n",
    "    l_matches.append({\n",
    "        'call_convrstn_id': row['call_convrstn_id'], 'chunk_id': row['chunk_id'], \n",
    "        'text': row['sntnce'], 'mover_match': ';'.join(l_move_match), \n",
    "        'cancel_match': ';'.join(l_cancel_match), 'holiday_match': ';'.join(l_holiday_match), \n",
    "        'expression_match': ';'.join(l_expression_match), 'things_match': ';'.join(l_things_match), \n",
    "        'negation_match': ';'.join(l_negation_regex)\n",
    "    })\n",
    "\n",
    "# format & save\n",
    "df_movers = pd.DataFrame(l_matches)\n",
    "df_movers.to_excel(pth_data / 'adhoc' / 'movers_match.xlsx', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4a2cbc-2d2e-4eb1-9311-ceab6191f450",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_conversations = pd.read_csv(pth_data / 'extract' / 'conversations.csv', index_col=None)\\\n",
    "    .loc[:, ['call_convrstn_id', 'bus_bacct_num', 'call_convrstn_date']]\n",
    "df_movers = df_movers.merge(df_conversations, on='call_convrstn_id', how='left')\n",
    "\n",
    "l_info = []\n",
    "for date, df_sub in df_movers.groupby('call_convrstn_date'):\n",
    "    l_info.append({\n",
    "        'date': date,\n",
    "        'cnt_unique_conv_id': len(df_sub['call_convrstn_id'].unique()), \n",
    "        'cnt_unique_ban': len(df_sub.loc[~df_sub['bus_bacct_num'].isnull(), 'bus_bacct_num'].unique())        \n",
    "    })\n",
    "                                  \n",
    "pd.DataFrame(l_info).to_excel(\n",
    "    pth_data / 'adhoc' / 'movers_info.xlsx', index=False\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207947cf-789b-47da-a15d-cc8525d0fecc",
   "metadata": {},
   "source": [
    "#### 3. extract examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062dd93d-2e6b-406e-bda8-3dcd53ea58e4",
   "metadata": {},
   "source": [
    "##### A. Get BQ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7ef415-4c5a-44b2-a37d-6de4c9c286ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_rate = 1000\n",
    "n_pass = int(len(df_examples) / batch_rate) + 1\n",
    "for i in range(n_pass):\n",
    "    df_sub = df_examples.iloc[i * batch_rate: (i+1) * batch_rate]\n",
    "\n",
    "    load_examples_to_datahub(\n",
    "        bq_client, df_sub, pth_queries, d_project_config['gcp-project-name'], \n",
    "        d_project_config['dataset'], d_params['labelling']['table_name']\n",
    "    )\n",
    "    time.sleep(1)\n",
    "    count = extract_bq_data(\n",
    "        bq_client, \n",
    "        '''SELECT count(*) \n",
    "        from `divg-pgspeech-pr-b8a291.divg_pgspeech_pr_dataset.examples_contract_end`'''\n",
    "    )\n",
    "    print(f'Count of rows is {count.iloc[0, 0]}')\n",
    "    print(f'{i}-{i * batch_rate}-{(i+1) * batch_rate}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75445bb7-ec69-4ec3-bd96-448f66b86994",
   "metadata": {},
   "source": [
    "##### B. Export as excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af22a80-2341-4da9-8922-2f553b7445c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "df_examples = extract_bq_data(\n",
    "    bq_client, \n",
    "    '''SELECT *\n",
    "        from `divg-pgspeech-pr-b8a291.divg_pgspeech_pr_dataset.examples_mover`\n",
    "    '''\n",
    ")\n",
    "\n",
    "# Add ind and reformat dataframe\n",
    "df_examples = df_examples.assign(\n",
    "    text=lambda df: df['text'].str.replace('\\\\033\\[1m|\\\\033\\[0m', '')   \n",
    ")\n",
    "\n",
    "# Save ref & ano\n",
    "df_examples[['chunk_id', 'text']].to_excel(\n",
    "    pth_data / 'labelling' / 'movers_v0_ano.xlsx', index=False\n",
    ")  \n",
    "\n",
    "df_examples.to_csv(\n",
    "    pth_data / 'labelling' / 'movers_v0_ref.csv', index=False, sep=';'\n",
    ")\n",
    "df_ori_sentences.to_csv(\n",
    "    pth_data / 'labelling' / 'movers_v0_sentences.csv', index=False, sep=';'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acd5285-f521-4cc4-97a4-4ae3dd08bc4f",
   "metadata": {},
   "source": [
    "#### 4. Build dataset for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05e3e0d-298c-4208-9c8b-322372f3fa6d",
   "metadata": {},
   "source": [
    "##### A Extract annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243a32d8-3387-44ff-8b33-6f168278a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_annotations = pth_data / 'labelling' / 'training_data_movers.csv'\n",
    "df_annotations = pd.read_csv(pth_annotations, index_col=None, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a7bc65-b3c1-4fbd-8d04-3b8616b1e990",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l_sentences, d_labels = [], {}\n",
    "for _, row in df_annotations.iterrows():\n",
    "    if pd.isnull(row['label']):\n",
    "        continue\n",
    "    \n",
    "    # Update label's dict\n",
    "    label = 0 if row['label'] == 'N' else 1\n",
    "    d_labels[row['conv_id']] = d_labels.get(row['conv_id'], None) or {}\n",
    "    d_labels[row['conv_id']][str(row['chunk_id'])] = label\n",
    "    \n",
    "    # Update sentences \n",
    "    l_sentences.append({\n",
    "        \"sntnce\": row['sntnce'], 'call_convrstn_id': row['conv_id'], \n",
    "        'chunk_id': str(row['chunk_id']), 'sntnce_partcpnt_role': \"END_USER\"\n",
    "    })\n",
    "    \n",
    "df_sentences = pd.DataFrame(l_sentences)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0228a65-66f1-4f1a-80ea-e75392eaf08f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract model data\n",
    "d_regex, l_tags, d_stopwords, intent_detector = extract_all_model_data(\n",
    "    pth_util_data, split_regex=True, **d_params['model']\n",
    ")\n",
    "l_regex = [r for l_regs in d_regex.values() for r in l_regs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c585aebf-5917-4796-9f3b-9420a666df2a",
   "metadata": {},
   "source": [
    "##### B Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e022188c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute features\n",
    "mover_dataset = extract_convs_features(\n",
    "    df_sentences, d_regex, intent_detector, d_labels=d_labels, \n",
    "    d_stopwords=d_stopwords, l_tags=l_tags\n",
    ")\n",
    "\n",
    "# Save features\n",
    "with (pth_data / 'training' /'mover_dataset.pkl').open(mode='wb') as f:\n",
    "    pickle.dump(mover_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6c7da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some stats\n",
    "training_data_desc = f\"\"\"\n",
    "    Size of training dataset: {sum([len(d) for d in d_labels.values()])}\n",
    "    Size of target 0: {sum([len([v for v in d.values() if v == 0]) for d in d_labels.values()])}\n",
    "    Size of target 1: {sum([len([v for v in d.values() if v != 0]) for d in d_labels.values()])}\n",
    "    Feature dim: {mover_dataset.X.shape}\n",
    "\"\"\"\n",
    "print(training_data_desc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
